
["llama3.2_1b"]
    chat_template="llama3.2.jinja2"
    model_name="meta-llama/Llama-3.2-1B-Instruct"
    restricted_tokens=["128000-128255", "non-ascii"]
# idx between 128000 and 128255 (https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/blob/main/tokenizer_config.json)
["llama3.2_3b"]
    chat_template="llama3.2.jinja2"
    model_name="meta-llama/Llama-3.2-3B-Instruct"
    restricted_tokens=["128000-128255", "non-ascii"]
# idx between 128000 and 128255 (https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/blob/main/tokenizer_config.json)
["gemma2_2b"]
    chat_template="gemma2_2b.jinja2"
    model_name="google/gemma-2-2b-it"
    padding=false
    restricted_tokens=["0-108", "non-ascii"]
    system_message="none"
    # 108 first idx (https://huggingface.co/google/gemma-2-2b-it/blob/main/tokenizer_config.json)

["qwen2.5_1.5b"]

    chat_template="qwen2.5_1.5b.jinja2"
    model_name="Qwen/Qwen2.5-1.5B-Instruct"
    restricted_tokens=["non-ascii"]

["gpt"]
    model_name="gpt2"
